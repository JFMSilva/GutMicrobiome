---
title: "Human gut microbiome of children with ASD"
author: "Joao Fernando Marques"
date: "2022-11-19"
output: 
  pdf_document:
    fig_caption: yes
header-includes:
  \setlength\parindent{24pt}\setlength{\parskip}{0.0pt plus 1.0pt}
  \usepackage{float, indentfirst}
  \let\origfigure\figure
  \let\endorigfigure\endfigure
  \renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
  } {
    \endorigfigure
  }
---

```{r library, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'h')

# The code above in "header-includes" is from this page:
# https://stackoverflow.com/questions/16626462/figure-position-in-markdown-when-converting-to-pdf-with-knitr-and-pandoc/33801326#33801326
# the objective is to keep the figures in place when using "fig-cap" in the chunks options.

if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(cowplot)) install.packages("cowplot", repos = "http://cran.us.r-project.org")
if(!require(vegan)) install.packages("vegan", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra ", repos = "http://cran.us.r-project.org")
if(!require(reshape2)) install.packages("reshape2 ", repos = "http://cran.us.r-project.org")

library(data.table)
library(cowplot)
library(vegan)
library(tidyverse)
library(caret)
library(kableExtra)
library(reshape2)

# the file were downloaded from here:
# https://www.kaggle.com/datasets/antaresnyc/human-gut-microbiome-with-asd?select=GSE113690_Autism_16S_rRNA_OTU_assignment_and_abundance.csv
```

## Introduction

The dataset used in this report is from the research paper by Zhou Dan et al. published on April 21st of 2020 - [Altered gut microbial profile is associated with abnormal metabolism activity of Autism Spectrum Disorder](https://www.nature.com/articles/s41596-019-0264-1) downloaded from [kaggle.com](https://www.kaggle.com/datasets/antaresnyc/human-gut-microbiome-with-asd?select=GSE113690_Autism_16S_rRNA_OTU_assignment_and_abundance.csv). Autistic Spectrum Disorder (ASD) is a severe neurodevelopmental disorder that is primarily characterized by abnormal behavioral symptoms, and children with ASD often have gastrointestinal symptoms, such as gaseousness, diarrhea, and constipation. A few studies a small number of subjects have shown that individuals with ASD have different gut bacterial microbiota from typically developing (TD) individuals, and that early life perturbations of the developing gut microbiota can impact neurodevelopment and potentially lead to adverse mental health outcomes later in life through the gut-microbiome-brain axis. Thus, it still needs to be more thoroughly analyzed whether the abundance and diversity of altered bacteria indicates the potential interaction effects between ASD and gastrointestinal symptoms.

Classic methods of studying the bacterial microbiota have shortcomings, the main one being that not all bacteria can be cultured under standard laboratory conditions, but rapid advances in high-throughput sequencing technologies have profoundly changed the study of microbiomes across diverse environments. Next-generation sequencing (NGS) uses the sequence of 16S rRNA, a highly conserved region in bacterial genomes to identify bacteria within samples, but comes with several analytical challenges. The method generates various reads from the different 16S presents in the sample, generating data that is compositional, that is, only reflects the relative abundance of each bacteria in the sample. If a dominant bacteria increases in the sample, the proportion of all the others will decrease, even if their absolute abundance remain constant. Also, the number of reads per sample varies (read depth), so some form of data normalization is necessary, the data are often very sparse, and there are usually more predictors than samples. So these characteristics of microbiome data must be considered in statistical analyses.

Given this, the objective of this report is to classify individuals in ASD or TD based on their microbiome profile, as well to compare different machine-learning methods to random forest when used in microbiome data. Random forest was chosen as a start point because of the various machine-learning algorithm, it is able to identify non-linear relationships, deal with variable interactions, is robust to overfitting and works well with high dimensional data with low signal-to-noise ratio, such as microbiome data.


## Methods

### Data exploration

The raw file from kaggle has 1322 rows and 256 columns, but the predictors, the bacteria, are the rows. The first column identifies the bacteria by a short ID, called Operational Taxonomic Unit (OTU), the second column gives the taxonomic classification of each bacteria, and the other columns are the samples, with a code identifying if a sample is from ASD or TD individuals. In the paper is said that data were collected from 286 children, but there are only 254 samples in the kaggle database, so some bacteria that were only present in those missing 32 samples have 0 reads and have to be removed.

This file was separated in three data.frames, Data, Target and Taxonomy with the following code:
```{r echo=TRUE, results = 'hide'}
##### Read file #####
Raw <- fread("GSE113690_Autism_16S_rRNA_OTU_assignment_and_abundance.csv", 
             stringsAsFactors = T)

# Create Taxonomy data.frame, with OTU and Taxonomy columns
Taxonomy <- Raw %>% 
  select(OTU, taxonomy)

# Remove Taxonomy and OTU columns, 
# transpose the results and 
# rename the columns with the OTU column to cross-reference the taxonomy table.
# The rownames are then transformed into a column and the tags are 
# recoded as ASD or TD (According to the supplementary material of the paper).
# Any bacteria without reads are then removed from the object.
Data <- Raw %>% 
  select(-taxonomy, -OTU) %>% 
  t() %>% 
  data.frame() %>% 
  `colnames<-`(Raw$OTU) %>% 
  rownames_to_column("target") %>% 
  mutate(target = str_remove_all(target, "\\d"),
         target = factor(target),
         target = fct_recode(target, ASD = "B", TD = "A"),
         ) %>%
  select(target, where(~ is.numeric(.x) && sum(.x) != 0))

# A new data.frame is created with the target column and
# this column is removed from the Data object.
Target <- select(Data, target)
Data <- select(Data, -target)

# The bacteria present in the taxonomy object is filtered based on the remaining 
# bacteria in the dataset.
# The taxonomy column is separated in 8 columns, with each column representing 
# a taxonomy rank.
Taxonomy <- Taxonomy %>% 
  filter(OTU %in% colnames(Data)) %>% 
  separate(taxonomy, sep = ";_", 
           into = c("domain",
                    "kingdom",
                    "phylum", 
                    "class", 
                    "order",
                    "family", 
                    "genus", 
                    "species"))
```

After the data cleaning step, we ended with `r nrow(Data)` samples with `r ncol(Data)` different bacteria observed in these samples. Figure 1 shows the number of samples in each of the groups, ASD or TD. Since all samples have the same read depth (`r unique(rowSums(Data))` reads), we can use the abundance table direct in the analysis, otherwise we would need to process it further, rarefying the samples to equal read depth, or at least transforming it to relative abundance.

```{r echo=FALSE, fig.cap='Number of samples in each group', fig.height=2.8, fig.width = 9.1, out.height = "2in", out.width = "6.5in"}
Target %>% 
  count(target) %>% 
  ggplot(aes(x = target, y = n, fill = target, label = n)) +
  geom_col() + 
  geom_text(vjust = 1) +
  theme_bw() +
  xlab("Condition") +
  ylab("Number of samples") +
  theme(legend.position = "none")
```

We can explore relations of the bacteria observed with barplots of different taxonomy categories. Figure 2 shows the barplot for Phylum in each sample, but in this category rank it is difficult to see any difference, and is already difficult to see difference in the colors because of the number of phyla present.

```{r echo=FALSE, fig.cap='Stacked bar-plot representation of microbiota compositions with taxonomic features collapsed at the level of phyla', fig.height=8.48, fig.width = 10.4, out.height = "5.3in", out.width = "6.5in"}
# All three objects are united into one.
# first the data is converted to relative abundance,
# then it is melted to long format, suitable to ploting,
# then its joined to the taxonomy table
# and finally with the target table.
# We can summarise the resulting object to create compositional bar plots of different taxa ranks.

FullData <- Data %>% 
  apply(MARGIN = 1, function(x) { (x / sum(x))*100 }) %>% 
  t() %>% 
  data.frame() %>% 
  rownames_to_column("Sample") %>% 
  melt(id.vars = "Sample", variable.name = "OTU", value.name = "Abundance") %>% 
  left_join(Taxonomy, by = "OTU") %>% 
  left_join(rownames_to_column(Target, "Sample"), by = "Sample") 

FullData %>% 
  group_by(Sample, target, phylum) %>% 
  summarise(Abundance = sum(Abundance), .groups = "drop_last") %>% 
  ggplot(aes(x = Sample, y = Abundance, fill = phylum)) +
  geom_col() +
  facet_grid(~ target, scales = "free") +
  theme_bw() +
  theme(legend.position = "bottom",
        legend.key.size = unit(2, 'mm'),
        axis.text.x = element_blank()) +
  guides(fill=guide_legend(ncol=3)) +
  ylab("Relative Abundance (%)") +
  scale_y_continuous(expand = expansion(mult = c(0, 0)))


```

To inspect whether lower ranks show differences between groups we can use ordination maps. A widely used method is Principal Component Analysis (PCA), but here we come across another peculiarity of ecological data, not only for microbiome, but for all data that count species. These types of data are not euclidian, so we can't use any statistical method that relies in euclidian distance and have to use other methods. This happens because the value of "0" have a special meaning in ecological data. Think about three samples where you searched for a bird species. In sample "A" you did not see any birds, in sample "B" you saw 5 birds and in sample "C" you saw 150 birds. If we analyse these data with euclidian distance, sample "B" is much more similar to sample "A" than it is to sample "C" (distance from "B" to "A" is 5, and distance from "B" to "C" is 145), but for the real, living species, sample "B" is more similar to "C", because the species manages to survive in it, and does not survive in sample "A". Sample "A" could be from a desert, sample "B" from a forest edge, and sample C" from the forest interior. Figure 3 shows what happens if we apply PCA to the microbiome dataset.

```{r echo=FALSE, fig.cap='Principal Components Analysis of microbiome data. A: First and second principal components analysis axes. B: Third and fourth principal components analysis axes', fig.height = 6.88, fig.width = 10.4, out.height = "4.3in", out.width = "6.5in"}

Data.pca <- prcomp(Data)

PCA_1_2 <- Data.pca$x %>% 
  data.frame() %>% 
  mutate(target = Target$target) %>% 
  ggplot(aes(x = PC1, y = PC2, color = target)) +
  geom_point() +
  stat_ellipse() +
  theme_bw() +
  theme(legend.position = "bottom") +
  xlab(paste0("PC 1 (", round(100 * Data.pca$sdev[1]^2/sum(Data.pca$sdev^2), 1), "%)")) +
  ylab(paste0("PC 2 (", round(100 * Data.pca$sdev[2]^2/sum(Data.pca$sdev^2), 1), "%)"))

PCA_3_4 <- Data.pca$x %>% 
  data.frame() %>% 
  mutate(target = Target$target) %>% 
  ggplot(aes(x = PC3, y = PC4, color = target)) +
  geom_point() +
  stat_ellipse() +
  theme_bw() +
  theme(legend.position = "bottom") +
  xlab(paste0("PC 3 (", round(100 * Data.pca$sdev[3]^2/sum(Data.pca$sdev^2), 1), "%)")) +
  ylab(paste0("PC 4 (", round(100 * Data.pca$sdev[4]^2/sum(Data.pca$sdev^2), 1), "%)"))

# Plot two graphs in a grid, without legend
# then put these two in a new grid, over the legend
plot_grid(PCA_1_2 + theme(legend.position = "none"), 
          PCA_3_4 + theme(legend.position = "none"), 
          labels = c("A", "B"),
          ncol = 2) %>% 
  plot_grid(get_legend(PCA_1_2), 
            ncol = 1,
            rel_heights = c(0.9, 0.1))
  
```

To reduce the dimensionality of ecological data, we need to use dissimilarity measures, not Euclidean distances, to compare samples. Several dissimilarity indices are available, but the most common ones are the Jaccard dissimilarity, which considers the occurrence of species to quantify the similarity of the samples, and the Bray-Curtis dissimilarity, which also considers the abundance of species. We also need to use another ordination technique that accepts dissimilarity data. In this case we will use Principal Coordinate Analysis (PCoA). Compare the results of these ordination techniques (figure 4) with the PCA result (figure 3). We can see a clearer separation of the two groups, and when we compare the result from Jaccard dissimilarities with  Bray-Curtis, the first one appears to separate better the two groups in the first two axys. This indicates that the difference between them lies in the occurrence of less abundant bacteria, and that the more abundant ones, which contribute more to Bray-Curtis dissimilarities, are more similar between the two groups. 

```{r include=FALSE}

# function to perform PCoA, from the package vegan
# to calculate de jaccard dissimilarity, it is important to set the binary option to TRUE
# the option metaMDSdist automatic transforms the data if necessary and decreases the bias
# caused by the sparse data.
Data.pco.jaccard <- capscale(Data ~ 1, distance = "jaccard", binary = T, metaMDSdist = TRUE, add = TRUE)

# function scores extracts the coordinates from the ordination result
PCO.jaccard_1_2 <- scores(Data.pco.jaccard, display = "sites") %>% 
  as.data.frame() %>% 
  mutate(target = Target$target) %>% 
  ggplot(aes(x = MDS1, y = MDS2, color = target)) +
  geom_point() +
  theme_bw() +
  stat_ellipse() +
  xlab(paste0("PCoA 1 (", round(100 * Data.pco.jaccard$CA$eig[1]/sum(Data.pco.jaccard$CA$eig), 1), "%)")) +
  ylab(paste0("PCoA 2 (", round(100 * Data.pco.jaccard$CA$eig[2]/sum(Data.pco.jaccard$CA$eig), 1), "%)")) + 
  theme(legend.position = "bottom")

PCO.jaccard_3_4 <- scores(Data.pco.jaccard, display = "sites", choices = c(3, 4)) %>% 
  as.data.frame() %>% 
  mutate(target = Target$target) %>% 
  ggplot(aes(x = MDS3, y = MDS4, color = target)) +
  geom_point() +
  theme_bw() +
  stat_ellipse() +
  xlab(paste0("PCoA 3 (", round(100 * Data.pco.jaccard$CA$eig[3]/sum(Data.pco.jaccard$CA$eig), 1), "%)")) +
  ylab(paste0("PCoA 4 (", round(100 * Data.pco.jaccard$CA$eig[4]/sum(Data.pco.jaccard$CA$eig), 1), "%)")) 

# PCoA based on Bray-Curtis dissimilarity
Data.pco.bray <- capscale(Data ~ 1, distance = "bray", binary = F, metaMDSdist = TRUE, add = TRUE)

PCO.bray_1_2 <- scores(Data.pco.bray, display = "sites") %>% 
  as.data.frame() %>% 
  mutate(target = Target$target) %>% 
  ggplot(aes(x = MDS1, y = MDS2, color = target)) +
  geom_point() +
  theme_bw() +
  stat_ellipse() +
  xlab(paste0("PCoA 1 (", round(100 * Data.pco.bray$CA$eig[1]/sum(Data.pco.bray$CA$eig), 1), "%)")) +
  ylab(paste0("PCoA 2 (", round(100 * Data.pco.bray$CA$eig[2]/sum(Data.pco.bray$CA$eig), 1), "%)"))

PCO.bray_3_4 <- scores(Data.pco.bray, display = "sites", choices = c(3, 4)) %>% 
  as.data.frame() %>% 
  mutate(target = Target$target) %>% 
  ggplot(aes(x = MDS3, y = MDS4, color = target)) +
  geom_point() +
  theme_bw() +
  stat_ellipse() +
  xlab(paste0("PCoA 3 (", round(100 * Data.pco.bray$CA$eig[3]/sum(Data.pco.bray$CA$eig), 1), "%)")) +
  ylab(paste0("PCoA 4 (", round(100 * Data.pco.bray$CA$eig[4]/sum(Data.pco.bray$CA$eig), 1), "%)")) 

```

```{r echo=FALSE, fig.cap='Principal Coordinates Analysis of microbiome data. A: First and second principal coordinate analysis axes for Jaccard dissimilarity. B: Third and fourth principal coordinate analysis axes for jaccard dissimilarity. C: First and second principal coordinate analysis axes for Bray-Curtis dissimilarity. D: Third and fourth principal coordinate analysis axes for Bray-Curtis dissimilarity', fig.height = 12.8, fig.width = 10.4, out.height = "8in", out.width = "6.5in"}
plot_grid(PCO.jaccard_1_2 + theme(legend.position = "none"), 
          PCO.jaccard_3_4 + theme(legend.position = "none"), 
          PCO.bray_1_2 + theme(legend.position = "none"), 
          PCO.bray_3_4 + theme(legend.position = "none"), 
          ncol = 2, 
          labels = c("A", "B", "C", "D")) %>% 
  plot_grid(get_legend(PCO.jaccard_1_2), 
            ncol = 1,
            rel_heights = c(0.95, 0.05))
```

These results shows us that we can't use machine learning algorithms that relies in euclidian distances, because they won't be able to separate the two groups, but models that look for the co-occurrence of predictors have a good chance of correctly classifying individuals. This is one of the reasons that Random Forest works well with microbiome data, it looks for the best variables to split the data, and not the similarity between the samples.

### Modeling approach

The data was split in two groups, a test set with 20% of the samples and a train set. The test set will only be used to evaluate the models, and the models will be build trained with the train set, with 5-fold cross-validation with 80% of the data. Since the data are sparse, any bacteria not present in the train set will be removed from the analysis. Since for this dataset the PCoA based on Jaccard dissimilarity appears to separate the two groups, all models will also be applied in the dataset transformed in a presence/ausence table. a benefit of this transformation is that it will facilitate the use of euclidean distances, as all distances will be either 0 or 1. The models that we will use are the following:

```{r include=FALSE}
set.seed(2, sample.kind = "Rounding") # if using R 3.5 or earlier, remove the sample.kind argument

# Create index for each dataset
test_index <- createDataPartition(Target$target, times = 1, p = 0.2, list = FALSE)

# Create train set
train_set <- Data[-test_index, ]
train_set_y <- Target$target[-test_index] 

# Create test set
test_set <- Data[test_index, ]
test_set_y <- Target$target[test_index] 


# Remove bacteria not present in the train set
train_set <- train_set[,colSums(train_set) != 0]
test_set <- test_set[, colnames(test_set) %in% colnames(train_set)]

# Transforme to presence/ausence
train_set_PA <- decostand(train_set, "pa")
test_set_PA <- decostand(test_set, "pa")

# Train method for the models: 5-fold cross validation
control <- trainControl(method = "cv", number = 5, p = 0.8)
```

#### Random Forest (Rborist):

This algorithm builds several decision trees, and presents the class selected by most of the trees as a result for each sample. In this way, the result of this algorithm is a majority vote ensemble of several models.

#### Extreme Gradient Boosted Decision Trees (xgbTree):

Like RF, Extreme Gradient Boosted Decision Trees is an ensemble of models, but each new model is built to predict the errors of the previous model. In this way, each new decision tree in the model improves the predictions of the previous model.

#### Neural Networks with a Principal Component Step (pcaNNet):

Neural networks involves a number of processors operating in parallel and arranged in tiers, with each tier operating in the output of the previous tier. This specific method performs a PCA previous to the model construction to reduce the dimensionality of the data, so it can be applied to datasets with more features than samples. But since it relies on PCA, it probably won't be efficient for microbiome data.

#### Logistic regression (glm):

A simple model for predicting binary outcomes, it is an extension of linear regression that assures that the estimate is between 0 and 1, using the logistic transformation for this. As the data are not normal and the variables are not independent, this model is likely to perform poorly.

#### Naive Bayes (naive_bayes):

A model similar to logistic regression, it uses Bayesâ€™ theorem to find the probability of a class given the probability of a feature.

#### k-nearest neighbors (knn):

This model calculates the distance between the samples to find the neighborhood that a sample belongs. Since it relies on distance between the features, is another model that is likely to perform poorly.

```{r include=FALSE}

Models <- c("Rborist", "xgbTree", "pcaNNet", "glm", "naive_bayes", "knn")

fits <- NULL
fits_PA <- NULL
Y_hats <- NULL
Y_hats_PA <- NULL
Performance <- NULL
for(i in Models) {
  
  #### Train models in the raw data
  
  # Set seed
  # set.seed(2) #if you are using R 3.5 or earlier
  set.seed(2, sample.kind = "Rounding") 
  
  # keep track of starts and end times
  start_time <- Sys.time()
  fits[[i]] <- train(x = train_set,
                     y = train_set_y,
                     method = i, 
                     trControl = control)
  end_time <- Sys.time()
  
  # make predictions in the test set
  Y_hats[[i]] <- predict(fits[[i]], test_set)
  
  # Calculate and save performancy metrics
  CM <- confusionMatrix(Y_hats[[i]],
                        test_set_y, 
                        positive = "ASD")
  
  
  
  #### Train models in the PA data
  
  # Set seed
  # set.seed(2) #if you are using R 3.5 or earlier
  set.seed(2, sample.kind = "Rounding") 
  
  # keep track of starts and end times
  start_time_PA <- Sys.time()
  fits_PA[[i]] <- train(x = train_set_PA,
                        y = train_set_y,
                        method = i, 
                        trControl = control)
  end_time_PA <- Sys.time()
  
  # make predictions in the test set
  Y_hats_PA[[i]] <- predict(fits_PA[[i]], test_set_PA)
  
  # Calculate and save performancy metrics
  CM_PA <- confusionMatrix(Y_hats_PA[[i]],
                           test_set_y, 
                           positive = "ASD")
  
  
  
  # final performance data:
  Performance[[i]] <- list(Accuracy = CM$overall[["Accuracy"]],
                           Accuracy_PA = CM_PA$overall[["Accuracy"]],
                           Sensitivity = CM$byClass[["Sensitivity"]],
                           Sensitivity_PA = CM_PA$byClass[["Sensitivity"]],
                           Specificity = CM$byClass[["Specificity"]],
                           Specificity_PA = CM_PA$byClass[["Specificity"]],
                           Time = as.numeric(difftime(end_time, start_time, units = "secs")),
                           Time_PA = as.numeric(difftime(end_time_PA, start_time_PA, units = "secs")))
}
```

## Results

The performance metrics of all the models are presented in table 1 and figure 5. As expected, the tree-based models were the ones with the highest accuracy, with Extreme Gradient Boosted Decision Trees surpassing Random Forest. All models that expect normal distribution of data and independence of variables had low accuracy, with Neural Networks with a Principal Component Step presenting the best result among them. In general, the specificity was greater than the sensitivity, which indicates that the models had greater ease in classifying TD children as TD, but some ASD children were classified as TD.

```{r echo=FALSE}
# transform the Performance list into a data frame,
# round all numbers to 3 decimal places
# use kable to create a table

rbindlist(Performance, idcol = "model")%>% 
  mutate(across(where(is.numeric), ~ round(.x, 3) )) %>%  
  kbl(booktabs = T, 
      linesep = "", 
      align = "c",
      col.names = c("", "Raw", "P/A", "Raw", "P/A", "Raw", "P/A", "Raw", "P/A"),
      caption = "Performance metrics of the six different models trained in the microbiome dataset.") %>% 
  add_header_above(c("", "Accuracy" = 2, "Sensitivity" = 2, "Specifcity" = 2, "Time (seconds)" = 2)) %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

The transformation of the dataset to presence/ausence did not interfere with tree-based models, but greatly improved the two models based in distance, Neural Networks with a Principal Component Step and k-nearest neighbors. k-nearest neighbors even reached the same accuracy score as Extreme Gradient Boosted Decision Trees, but training in only one second, versus the ~50 seconds that xgbTree took to train.


```{r echo=FALSE, fig.cap='Performance metrics of the tested models', fig.height = 5.85, fig.width = 8.45, out.height = "4.5in", out.width = "6.5in"}
# transform the Performance list into a data frame,
# convert to long format
# then organize the columns to plot
rbindlist(Performance, idcol = "model") %>% 
    melt(id.vars = "model") %>% 
    mutate(Data = case_when(str_detect(variable, "_PA") ~ "PA",
                            TRUE ~ "Raw"),
           variable = str_remove_all(variable, "_PA"),
           variable = case_when(str_detect(variable, "Time") ~ paste(variable, "(seconds)"),
                                TRUE ~ paste(variable, "(%)")),
           model = factor(model, levels = unique(model))) %>% 
    ggplot(aes(x = model, y = value, color = Data, group = Data)) +
    geom_point() +
    geom_line() +
    facet_wrap(~ variable, scales = "free") +
    theme_bw() +
    theme(legend.position = "bottom")
```

## Conclusion

These results showcase the power of decision trees in problematic data, such those of microbiome, with very sparse data and more features than samples. It also shows that with knowledge about how to transform the data, even simple models like knn can be applied with great accuracy, with only 5 samples misclassified.
```{r echo=FALSE}
confusionMatrix(Y_hats_PA$knn,
                           test_set_y, 
                           positive = "ASD")$table
```

A care that must be taken with this data is that it does not establish cause and effect. It is not an altered microbiome that leads to the development of ASD, but more likely the other way around. In the paper the authors also analysis changes in the microbiota with age, and found that ASD patients do not alter the microbiota with age, unlike TD children, whose microbiota changes over time.











